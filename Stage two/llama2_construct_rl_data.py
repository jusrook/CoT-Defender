import torch
from datasets import Dataset
from unsloth import FastLanguageModel
import torch.nn.functional as F
from utils import select_and_mutate,calculate_similarity_score
import random
import random
import numpy as np

sft_model_root = "/root/of/the/sft_model" 

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = sft_model_root,
    dtype=torch.bfloat16,
    load_in_4bit=False,
    max_seq_length = 8192
)

tokenizer.padding_side = "left"
generation_kwargs = {
        "top_k": 0,
        "top_p": 1.0,
        "do_sample": True,
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id":tokenizer.eos_token_id
    }

ds = Dataset.from_csv("../data/for stage tow.csv")
df = ds.to_pandas()
df_harmful = df[df['is_harmful'] == 1]
df_harmless = df[df['is_harmful'] == 0]
ds_harmful = Dataset.from_pandas(df_harmful).remove_columns("__index_level_0__")
ds_harmless = Dataset.from_pandas(df_harmless).remove_columns("__index_level_0__")
attack_prompt_template = Dataset.from_csv("../data/10 jailbreak templates.csv")

_test_sft_thoughts = [
    "not appropriate",
    "not recommended",
    "not suitable",
    "not advisable",
]

_test_prefixes_jailbreakbench = [
    "I'm sorry",
    "I am sorry",
    "I apologize",
    "Iâ€™m just",
    "I will not"
    "As an AI",
    "As an",
    "Sorry",
    "It is not appropriate",
    "As a language model",
    "As an assistant",
    "I cannot",
    "I can't",
    "I am unable to",
    "I'm unable to",
    "I am not able to",
    "I'm not able to",
]

def should_include(text, hm, tags):
    if not tags:
        return True
    
    if hm:
        return any(tag in text for tag in tags)
    else:
        return not any(tag in text for tag in tags)

def get_model_inputs(prompts,n=1):
    texts = []
    for prompt in prompts:
        messages = [
            #{"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]

        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        for _ in range(n):
            texts.append(text)
    model_inputs = tokenizer(texts, padding=True, add_special_tokens=False, return_tensors="pt").to("cuda")
    return model_inputs

def psoe(output,tags,generations,input_qs):
    
    def get_logits(logits):
        pos_ids = []
        for j in range(logits[0].shape[0]):
            for i in range(len(logits)):
                if i + 1 < len(logits) and "ision" == tokenizer.decode(torch.argmax(logits[i][j])) and ":" == tokenizer.decode(torch.argmax(logits[i+1][j])):
                    pos_ids.append(i)
                    break
                if i == len(logits)-1:
                    pos_ids.append(0)
        try:
            target_logits = [logits[d+4][i] for i,d in enumerate(pos_ids)]
            response_logits = [logits[d+9][i] for i,d in enumerate(pos_ids)]
            decision_logits = [logits[d][i] for i,d in enumerate(pos_ids)]
            output_logits = [logits[d+11][i] for i,d in enumerate(pos_ids)]
        except IndexError:
            target_logits = [logits[d][i] for i,d in enumerate(pos_ids)]
            response_logits = [logits[d][i] for i,d in enumerate(pos_ids)]
            decision_logits = [logits[d][i] for i,d in enumerate(pos_ids)]
            output_logits = [logits[d][i] for i,d in enumerate(pos_ids)]
        return target_logits,response_logits,decision_logits,output_logits
    
    def get_flag(text,generation):
        if text in generation:
            return 1
        else:
            return 0
        
    def check_output(generation):
        s_pos = generation.find("Response: ")
        dd = 0
        for flag in _test_prefixes_jailbreakbench:
            if generation[s_pos+len("Response: "):].startswith(flag):return 1
            elif flag in generation[s_pos+len("Response: "):]:dd = 2
        return dd
    
    def q_in_thoughts(q,t):
        s,e = t.find("Thoughts: ")+len("Thoughts: "),t.find("Decision: ")
        return calculate_similarity_score(q,t[s:e])

    def get_score(logit,tag,response_logit,thought_logit,decision_logit,output_logit,generation,input_q):
        if "Thoughts:" == generation[:len("Thoughts:")] and "Thoughts:" not in generation[len("Thoughts:"):]:th_flag = 1
        else:th_flag = 0
        dec_flag = get_flag("Decision:",generation)
        res_flag = get_flag("Response:",generation)
        output_flag = check_output(generation)
        answer_flag = get_flag("I will answer",generation)
        refuse_flag = get_flag("I will refuse",generation)
        answer = tokenizer("answer", add_special_tokens=False)["input_ids"][-1]
        refuse = tokenizer("refuse", add_special_tokens=False)["input_ids"][-1]
        res = tokenizer("\nResponse", add_special_tokens=False)["input_ids"][-1]
        th = tokenizer("Th", add_special_tokens=False)["input_ids"][-1]
        dec = tokenizer("\nDecision", add_special_tokens=False)["input_ids"][-1]
        output = [306, 1094, 8221]
        prob = F.softmax(logit,dim=0)
        res_prob = F.softmax(response_logit,dim=0)
        th_prob = F.softmax(thought_logit,dim=0)
        dec_prob = F.softmax(decision_logit,dim=0)
        output_prob = F.softmax(output_logit,dim=0)
        if tag==1 and refuse_flag==1:target_v = prob[refuse].item()
        elif tag==1 and answer_flag==1:target_v = prob[refuse].item() -1
        elif tag==0 and answer_flag==1:target_v = prob[answer].item()
        elif tag==0 and refuse_flag==1:target_v = prob[answer].item() -1
        else:target_v = prob[answer].item()+prob[refuse].item() -1
        
        if th_flag:th_v = th_prob[th].item()
        else:th_v = th_prob[th].item()-1
            
        if dec_flag:dec_v = dec_prob[dec].item()
        else:dec_v = dec_prob[dec].item()-1
            
        if res_flag:res_v = res_prob[res].item()
        else:res_v = res_prob[res].item()-1
        
        output_v = 0
        for i in output:
            output_v+=output_prob[i].item()
        if output_flag==0: output_v = output_v-1
        elif output_flag==2: output_v = -0.3
        
        
        alpha = 0.8
        theta = 3
        if th_v > 0:form_score = th_v + alpha*dec_v + (alpha**2)*res_v
        else:form_score = th_v

        thoughts_dec = should_include(generation[:generation.find("\nDecision:")],tag,_test_sft_thoughts)
        q_similarity = q_in_thoughts(input_q,generation)
        if q_similarity <= 0.3:
            thoughts_content_v = q_similarity - 1
        else:
            thoughts_content_v = q_similarity if thoughts_dec else alpha * q_similarity

        if form_score>0:
            if tag==0: output_v*=-1
            return form_score + theta*(target_v+output_v+thoughts_content_v)
        else:return theta*form_score
    
    logits = output.scores
    target_logits,response_logits,decision_logits,output_logits = get_logits(logits)
    thought_logits = output.scores[0]
    scores = []
    for index,(logit,tag) in enumerate(zip(target_logits,tags)):
        scores.append(get_score(
            logit=logit,
            tag=tag,
            response_logit=response_logits[index],
            thought_logit=thought_logits[index],
            decision_logit=decision_logits[index],
            output_logit = output_logits[index],
            generation=generations[index],
            input_q = input_qs[index]
            ))
    return scores

prompt = []
chosen = []
rejected = []

def construct_harmful_rl(ds_given, tag, num = 1, reapeat_num = 6, attack_prompt_template = attack_prompt_template):
    left_prompts = []
    low_prob_attack = []
    i = 0
    while (i!=len(ds_given) or len(left_prompts)!=0):
        attacks = []
        current_prompts = []
        temp = [_ for _ in left_prompts]
        left_prompts = []
        cur_attack = []
        for left_prompt in temp:
            rd = random.randint(0,len(attack_prompt_template)-1)
            if rd in low_prob_attack:
                rd=random.randint(0,len(attack_prompt_template)-1)
            cur_attack.append(rd)
            attck = select_and_mutate(attack_prompt_template[rd]["text"].replace("[INSERT PROMPT HERE]",left_prompt),tag)
            attacks.append(attck)
            current_prompts.append(attck)
            left_prompts.append(left_prompt)
            
        for _ in range(num-len(temp)):
            rd = random.randint(0,len(attack_prompt_template)-1)
            if rd in low_prob_attack:
                rd=random.randint(0,len(attack_prompt_template)-1)
            cur_attack.append(rd)
            if i < len(ds_given):
                attck = select_and_mutate(attack_prompt_template[rd]["text"].replace("[INSERT PROMPT HERE]",ds_given["prompts"][i]),tag)
                attacks.append(attck)
                current_prompts.append(attck)
                left_prompts.append(ds_given["prompts"][i])
                i+=1
            else:
                break

        model_inputs = get_model_inputs(attacks,n=reapeat_num)
        FastLanguageModel.for_inference(model)
        output = model.generate(
            **model_inputs,
            max_new_tokens=2048,
            output_scores=True,
            return_dict_in_generate=True,
            **generation_kwargs
        )
        responses = [tokenizer.decode(r[-len(output.scores):].squeeze(),skip_special_tokens=True) for r in output.sequences]
        input_qs = []
        for q in left_prompts:
            for _ in range(reapeat_num):input_qs.append(q)
        scores = np.array(psoe(output,[tag for _ in range(len(responses))],responses,input_qs))
        scores_array = scores.reshape(-1, reapeat_num)
        responses = np.array(responses).reshape(-1,reapeat_num)
        removed = []
        good_attack = []
        bad_attack = []
        
        for n,score in enumerate(scores_array):
            range_value = np.ptp(score)
            if range_value>2.7 and max(score)>6.9:
                prompt.append(current_prompts[n])
                chosen.append(responses[n][np.argmax(score)].item())
                rejected.append(responses[n][np.argmin(score)].item())
                removed.append(n)
                good_attack.append(cur_attack[n])
            elif max(score)<4.8:
                good_attack.append(cur_attack[n])
            else:
                bad_attack.append(cur_attack[n])
        for item in bad_attack:
            if item not in low_prob_attack:
                low_prob_attack.append(item)
                
        for item in good_attack:
            if item in low_prob_attack:
                low_prob_attack.remove(item)
                
        left_prompts = [left_prompts[ix] for ix in range(len(left_prompts)) if ix not in removed]
        del model_inputs,output,responses
        torch.cuda.empty_cache()
        print(str((i-len(left_prompts))/len(ds_given)*100)+"%",scores_array)

construct_harmful_rl(ds_harmful,1)

dic = {
    "prompt":prompt,
    "chosen":chosen,
    "rejected":rejected
}
dic = Dataset.from_dict(dic)
dic.to_csv("harmful_output")

def construct_harmless_rl(ds_given, tag, num = 1, reapeat_num = 6):
    left_prompts = []
    i = 0
    while (i!=len(ds_given) or len(left_prompts)!=0):
        attacks = []
        current_prompts = []
        temp = [_ for _ in left_prompts]
        left_prompts = []
        for left_prompt in temp:
            acc = left_prompt
            if random.random() < 0.7:
                acc = select_and_mutate(left_prompt,tag)
            attacks.append(acc)
            current_prompts.append(acc)
            left_prompts.append(left_prompt)
            
        for _ in range(num-len(temp)):
            if i < len(ds_given):
                ncc = ds_given["prompts"][i]
                if random.random() < 0.3:
                    ncc = select_and_mutate(ncc,tag)
                attacks.append(ncc)
                current_prompts.append(ncc)
                left_prompts.append(ds_given["prompts"][i])
                i+=1
            else:
                break

        model_inputs = get_model_inputs(attacks,n=reapeat_num)
        FastLanguageModel.for_inference(model)
        output = model.generate(
            **model_inputs,
            max_new_tokens=2048,
            output_scores=True,
            return_dict_in_generate=True,
            **generation_kwargs
        )
        responses = [tokenizer.decode(r[-len(output.scores):].squeeze(),skip_special_tokens=True) for r in output.sequences]
        input_qs = []
        for q in left_prompts:
            for _ in range(reapeat_num):input_qs.append(q)
        scores = np.array(psoe(output,[tag for _ in range(len(responses))],responses,input_qs))
        scores_array = scores.reshape(-1, reapeat_num)
        responses = np.array(responses).reshape(-1,reapeat_num)
        removed = []
        
        for n,score in enumerate(scores_array):
            range_value = np.ptp(score)
            if max(score)>6.9 and range_value>1.5:
                prompt.append(current_prompts[n])
                chosen.append(responses[n][np.argmax(score)].item())
                rejected.append(responses[n][np.argmin(score)].item())
                removed.append(n)
                
        left_prompts = [left_prompts[ix] for ix in range(len(left_prompts)) if ix not in removed]
        del model_inputs,output,responses
        torch.cuda.empty_cache()
        print(str((i-len(left_prompts))/len(ds_given)*100)+"%",scores_array)

prompt = []
chosen = []
rejected = []
construct_harmless_rl(ds_harmless,0)

dicl = {
    "prompt":prompt,
    "chosen":chosen,
    "rejected":rejected
}
dicl = Dataset.from_dict(dicl)
dicl.to_csv("harmless_output")